---
title: "FLOW"
author: "Francesco Cabras"
date: "21 April 2020"
output:
  html_document:
    df_print: paged
---

```{r, echo=F}
library(ggpubr)
library(car)
library(eurostat)
library(HDCI)
library(caret)
library(scales)
library(glmnet)
library(corrplot)
library(wbstats)
library(lmtest)
library(sandwich)
library(robustbase)
```

```{r}
D <- read.csv("DAT.csv")
```

Simple linear model.

```{R}
LM <- lm(log(PRI) ~ log(GAS) + DEP + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR, D)
summary(LM)
```

In the case of a random effects model, we find that, interestingly REN is strongly significant and has a negative effect on the price. Instead, when we deal with a fixed effects model, the effect is not significant but with a positive coefficient. This may result from the fact that countries with high share of renewable energy have a lower dummy.

We may want to use part of the geographical information by including a dummy variable that indicates the belonging to the European Union (i.e. being one of the 28 member countries).

```{r}
for (row in 1:nrow(D)) {
  if (as.character(D[row, "geo"]) %in% c("BE", "EL", "LT", "PT", "BG", "ES", "LU", "RO", "CZ", "FR", "HU", "SI", "DK", "HR", "MT", "SK", "DE", "IT", "NL", "FI", "EE", "CY", "AT", "SE", "IE", "LV", "PL", "UK")) {
    D$EU[row] <- 1
  }
  else { D$EU[row] <- 0 }
}


LMe <- lm(log(PRI) ~ log(GAS) + DEP + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR + EU, D)
summary(LMe)
```

Also dependency becomes significant when we control for EU belonging.

```{r}
DP <- D

for (row in 1:nrow(DP)) {
  if (as.character(DP[row, "geo"]) %in% c("BG", "RO", "CZ", "HU", "HR", "SE", "DK", "PL", "UK")) {
    DP$EU[row] <- "EU"
}
  else if (as.character(DP[row, "geo"]) %in% c("NO", "IS", "LI")) {
    DP$EU[row] <- "EFTA"
  }
  else if (as.character(DP[row, "geo"]) %in% c("BE", "EL", "LT", "PT", "ES", "LU", "FR", "SI", "MT", "SK", "DE", "IT", "NL", "FI", "EE", "CY", "AT", "IE", "LV")){
    DP$EU[row] <- "EA"
    }
}
rm(row)

DP$EU <- as.factor(DP$EU)
LMeP <- lm(log(PRI) ~ log(GAS) + DEP + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR + EU, DP)
summary(LMeP)
```

To be checked:
- residuals sum to zero
- no autocorrelation for residuals
- no perfect multicollinearity
- homoskedasticity

```{r}
par(mfrow = c(2, 2))
plot(LMeP)
```

What happens with observation 956 and 996?

```{r}
D[c(956,996),]
```

Price is very low because Ukraine may be considered as a not yet very developed country where, also, energy is very easily obtainable. Nothing too weird here, we do not consider them as outliers.

```{r}
mean(LMeP$residuals)
```

The mean of the residuals is approximately zero.

```{r}
lawstat::runs.test(LMeP$residuals)
```

There seems to be no problem with autocorrelation of residuals.

```{r}
vif(LMeP)
```

Variance Inflator Formula also ok.

```{r}
plot(LMeP,1)
```

The lower the fitted value, the higher the variance in the residual: we detect a certain amount of heteroskedasticity, confirmed by the value of the bptest.

To try to solve it, box-cox transformation of the dependent variable does not solve the problem.

```{r}
S <- DP
U <- caret::BoxCoxTrans(S$PRI)
S <- cbind(S, nP=predict(U, S$PRI))
LMeB <- lm(nP ~ log(GAS) + DEP + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR + EU, S)
bptest(LMeB)
rm(S, U)
```

Let us deviate from ordinary least squares to use a weighted least squares model.

```{r}
DP$resi <- LMeP$residuals
LMeR <- lm(log(resi^2) ~ log(GAS) + DEP + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR + EU, DP)

DP$varFunc <- exp(LMeR$fitted.values)
LMgls <- lm(log(PRI) ~ DEP + log(GAS) + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR + EU, weights = 1/sqrt(varFunc), DP)
summary(LMgls)
```

Performance slightly improved as measured by r squared, however let us check plot and bp test.

```{r}
plot(LMgls)
bptest(LMgls)
```

Bad news. Same result if we use an already-prepared robust linear model package.

```{r}
lmrobfit <- lmrob(log(PRI) ~ DEP + log(GAS) + log(CON) + INF + REN + log(GDP) + log(OIL) + MAR + EU, DP)
bptest(lmrobfit)
rm(lmrobfit)
```

Bell-shape form of the dependent variable.

```{r}
ggdensity(log(DP$PRI), main = "Density function PRI")
```

Let us

```{r}
qqnorm(residuals(LMgls), ylab="Residuals")
qqline(residuals(LMgls))
hist(residuals(LMgls))
shapiro.test(residuals(LMgls))
```

Subset selection. We want to maximize adjusted R squared and minimize cp and bic.

```{R}
library(leaps)
LM3 <- regsubsets(sqrt(PRI) ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, D, nvmax = 36)
which.max(summary(LM3)$adjr2)
which.min(summary(LM3)$bic)
which.min(summary(LM3)$cp)
```

Optimal values for adjusted R squared and cp are obtained with 32 values while 27 variables are enough for bic. MAR, NUC and OIL are dropped. Rather than using proxies for test error, let us directly compute this error through validation set approach.

```{R}
train <- (D$time < 2015)
test <- (!train)
```

Simple linear regression on train now. To compare accuracies of models we are going to use Mean Absolute Error (for interpretability reasons). Since our dependent variable is measured on a square root scale we have to have to square prediction vectors.

```{R}
tLM <- regsubsets(sqrt(PRI) ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, D, subset=train, nvmax = 36)
test.mat <- model.matrix(sqrt(PRI) ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, data=D[test,])
val.errors <- rep(NA, 36)
for (i in 1:36){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean(abs(D$PRI[test] - pred^2))
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))
```

Now, time for ridge regression.

```{r}
x <- model.matrix(sqrt(PRI) ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, D)[,-1]
y <- D$PRI
set.seed(35)
ridge <- cv.glmnet(x[train,], sqrt(y[train]), alpha=0, standardize=T)
bestlam <- min(ridge$lambda)
```

What is the test average difference between the prediction and the actual price of electricity?

```{r}
ridge.pred <- predict(ridge, s=bestlam, newx=x[test,])
mean(abs((ridge.pred)^2 - y[test]))
```

Coefficients.

```{r}
predict(ridge, type="coefficients", s=bestlam)[1:25,]
```

Time for lasso.

```{r}
lasso.mod <- glmnet(x[train,], sqrt(y[train]), alpha=1)
plot(lasso.mod)
```

Let us look for the best value of the parameter lambda (shrinkage).

```{r}
set.seed(35)
cv.out <- cv.glmnet(x[train,], sqrt(y[train]), alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean(abs(pred^2 - y[test]))
```

How many features is the model including?

```{r, warning=F}
C <- as.vector(coef(cv.out, bestlam))
length(C[C != 0])
```

All features are kept in the model (weird?).

```{r}
set.seed(35)
obj.escv <- escv.glmnet(x[train,], sqrt(y[train]))
obj <- Lasso(x[train,], sqrt(y[train]), lambda = obj.escv$lambda.cv)
pred <- mypredict(obj, newx = x[test,])
mean(abs((pred)^2 - y[test]))
```

Similar result with different algorithm, so maybe not that weird.

Let us instead measure performance of a simple linear model with only the geographical component.

As already explained, square root transformation is used because lighten the weight on the model of some high leverage points. But let's see what happens with a lasso with no log transformation applied.

```{r}
x <- model.matrix(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, D)[,-1]
lasso.mod <- glmnet(x[train,], y[train], alpha=1)
set.seed(35)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean(abs(pred - y[test]))
```

In this case, we would have a mean absolute error, which would outperform all previous algorithms.

However, even better, simple lm with variables through with subset selection:

```{r}
tLM <- regsubsets(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, D, subset=train, nvmax = 36)
test.mat <- model.matrix(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + geo + NUC + CRI, data=D[test,])
val.errors <- rep(NA, 36)
for (i in 1:36){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean(abs(D$PRI[test] - pred))
}
which.min(val.errors)
val.errors[which.min(val.errors)]
```

A sign that maybe that maybe those log transformation could be not best decision.
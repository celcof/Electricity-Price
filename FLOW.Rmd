---
title: "STAT"
author: "Francesco Cabras"
date: "21 April 2020"
output:
  html_document:
    df_print: paged
---
```{r, echo=F}
library(car)
library(eurostat)
library(HDCI)
library(caret)
```

Take data from library eurostat. Start from the dependent variable.

```{r}
X <- get_eurostat("ten00117", time_format = "num")
X <- X[grep("MSHH", X$indic_en), c(5:7)]
X <- X[!(X$geo %in% c("EU28", "NO", "TR", "EA", "EU27_2020", "AL", "BA", "GE", "IS", "LI", "MD", "ME", "MK", "RS", "UA", "XK")),]
names(X)[3] <- "PRI"
X$geo <- as.factor(as.character(X$geo))
```

Dependency from imports.

```{r}
I <- get_eurostat("nrg_ind_id", time_format = "num")
I <- I[grep("TOTAL", I$siec), ]
I <- I[,c(3:5)]
names(I)[3] <- "DEP"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Consumption of electricity.

```{r}
I <- get_eurostat("sdg_07_20", time_format = "num")
I <- I[, -1]
names(I)[3] <- "CON"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Real gdp per capita.

```{r}
I <- get_eurostat("sdg_08_10", time_format = "num")
I <- I[grep("CLV10_EUR_HAB", I$unit), ]
I <- I[, -c(1,2)]
names(I)[3] <- "GDP"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Oil price per barrel in dollars.

```{r}
time <- c(2008:2019)
OIL <- c(99.67,61.95,79.48,94.88,94.05,97.98,93.17,48.72,43.58,50.84,64.90,57.05)
I <- data.frame(time, OIL)
X <- merge(X, I, by="time", all.x=TRUE)
```

Share of renewable energy consumed.

```{r}
I <- get_eurostat("sdg_07_40", time_format = "num")
I <- I[I$nrg_bal == "REN", ]
I <- I[, -c(1,2)]
names(I)[3] <- "REN"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Nuclear energy production.

```{r}
search_eurostat("nuclear")$code
I <- get_eurostat("nrg_inf_nuc", time_format = "num")
I <- I[grep("PRD_NUCH", I$plant_tec), ]
I <- I[,c(3:5)]
names(I)[3] <- "NUC"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Price index

```{r}
I <- get_eurostat("prc_hicp_aind", time_format = "num")
I <- I[grep("CP00", I$coicop), ]
I <- I[grep("INX_A_AVG", I$unit), c(3:5)]
names(I)[3] <- "CPI"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Emissions intensity.

```{r}
I <- get_eurostat("sdg_13_20", time_format = "num")
I <- I[,-1]
names(I)[3] <- "EMI"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

share of largest electricity generator in the market.

```{r}
I <- get_eurostat("ten00119", time_format = "num")
I <- I[,c(4:6)]
names(I)[3] <- "MAR"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

substitute NA with zero for nuclear capacity

```{r}
for (row in 1:length(X$NUC)){
  if(X$time[row] %in% c(2009:2018) & is.na(X$NUC[row])){
    X$NUC[row] <- 0
  }
}
for (country in levels(X$geo)){
  X[X$time == 2008 & X$geo == country, "NUC"] <- X[X$time == 2009 & X$geo == country, "NUC"]
}
```

substitute NA for Market Concentration

of Bulgaria. we miss values until 2012: we just substitute with first value available (2013)

```{r}
X[X$geo == "BG" & X$time %in% c(2008:2012), "MAR"] <- X[X$geo == "BG" & X$time == 2013, "MAR"]
```

Of Germany. goes from 28.4 in 2010 to 32.0 in 2013. we expect linear behaviour

```{r}
in2011 <- 28.4 + (32 - 28.4) / 3
in2012 <- in2011 + (32 - 28.4) / 3
X[X$geo == "DE" & X$time == 2011, "MAR"] <- in2011
X[X$geo == "DE" & X$time == 2012, "MAR"] <- in2012
```

Of Greece. in 2011, average between previous and subsequent years.

```{r}
X[X$geo == "EL" & X$time == 2011, "MAR"] <- mean(c(X[X$geo == "EL" & X$time == 2010, "MAR"], X[X$geo == "EL" & X$time == 2012, "MAR"]))
```

Of Luxembourg. We miss values until 2009: we just substitute with first value available (2010)

```{r}
X[X$geo == "LU" & X$time %in% c(2008:2009), "MAR"] <- X[X$geo == "LU" & X$time == 2010, "MAR"]
```

Of Austria. We miss values until 2010: we just substitute with first value available (2011). Also miss values from 2014 on. substitute with ones from 2013

```{r}
X[X$geo == "AT" & X$time %in% c(2008:2010), "MAR"] <- X[X$geo == "AT" & X$time == 2011, "MAR"]
X[X$geo == "AT" & X$time %in% c(2014:2018), "MAR"] <- X[X$geo == "AT" & X$time == 2013, "MAR"]
```

Of Netherlands: average year values from Belgium, Luxembourg and Germany. Also take a look at https://www.sciencedirect.com/science/article/pii/S0301421518308061

```{r}
for (i in 1:nrow(X)) {
  year <- X$time[i]
  if (X$geo[i] == "NL" && X$time[i] != 2019){
    DEvalue <- X[X$geo == "DE" & X$time == year, "MAR"]
    BEvalue <- X[X$geo == "BE" & X$time == year, "MAR"]
    LUvalue <- X[X$geo == "LU" & X$time == year, "MAR"]
    DKvalue <- X[X$geo == "DK" & X$time == year, "MAR"]
    NLvalue <- mean(c(DEvalue, BEvalue, LUvalue, DKvalue), na.rm=T)
    X[X$geo == "NL" & X$time == year, "MAR"] <- NLvalue
  }
  X$MAR <- round(X$MAR,1)
}
```

Of UK we miss values from 2014 on. substitute with ones from 2013.

```{r}
X[X$geo == "UK" & X$time %in% c(2014:2018), "MAR"] <- X[X$geo == "UK" & X$time == 2013, "MAR"]
```

Finally, sub na for EMI 2018 following the trend from 2016 and 2017 (value 2017 + (value 2017 - value 2016)).

```{r}
for (country in levels(X$geo)){
  X[X$time == 2018 & X$geo == country, "EMI"] <- X[X$time == 2017 & X$geo == country, "EMI"] + (X[X$time == 2017 & X$geo == country, "EMI"] - X[X$time == 2016 & X$geo == country, "EMI"])
}
```

Did not notice we also miss gdp poland for 2018. We assume that this value is following the trend of the last two years. value 2018 + diff(2017-2018)

```{r}
X[X$geo == "PL" & X$time == 2018, "GDP"] <- X[X$geo == "PL" & X$time == 2017, "GDP"] + (X[X$geo == "PL" & X$time == 2017, "GDP"] - X[X$geo == "PL" & X$time == 2016, "GDP"])
```

Missing too many values for 2019.

```{r}
D <- X[!X$time == 2019,]
```

Only country variability.

```{R}
COU <- lm(PRI ~ geo, D)
summary(COU)$r.squared
```

83.5% of the variability is due to differences between member states.
Let us also control for inflation, thinking it may suffice.

```{R}
COU <- lm(PRI ~ geo + CPI, D)
summary(COU)$r.squared
```
87.5% of variability explained.

Some simple linear model.

```{R}
LM1 <- lm(PRI ~ DEP + CON + GDP + OIL + REN + NUC + CPI + EMI + MAR + geo, D)
summary(LM1)
```

Possible problems with assumptions, however. For example, we expect nuclear production not to have a gaussian distribution, since most countries do not have nuclear reactors.

```{r}
plot(LM1)
```

Problem with some observations, specifically 116 and 144, which are records of Cyprus in 2012 and 2013.
In these two years the price of electricity skyrocketed for reasons that our model is not able to figure out (i.e., debt crisis).

```{r}
D[116,]
mean(D$PRI)
D[D$geo=="CY","PRI"]
D[c(116,144),]
```

To be checked:
- residuals sum to zero
- no autocorrelation for residuals
- no perfect multicollinearity

```{r}
mean(LM1$residuals)
lawstat::runs.test(LM1$residuals)
vif(LM1)
```

Focus on correlated variables.

```{r}
library(corrplot)
corrplot(cor(D[, 3:12]))
```

Correlation of GDP with Price and Quantity Consumed is understandable but why does NUC have such a high VIF value? Not clear.

Overall check.

```{r}
gvlma::gvlma(LM1)
```

Alternative. Take the log of PRI, CON, GDP, OIL and EMI and remove the two CY records.

```{r}
D <- D[!(D$time == 2012 & D$geo == "CY"),]
D <- D[!(D$time == 2013 & D$geo == "CY"),]
LM2 <- lm(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + CPI + log(EMI) + NUC + MAR + geo, D)
gvlma::gvlma(LM2)
```

Better behaviour.

Subset selection. We want to maximize adjusted R squared and minimize cp and bic.

```{R}
library(leaps)
LM3 <- regsubsets(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + NUC + CPI + log(EMI) + MAR + geo, D, nvmax = 36)
which.max(summary(LM3)$adjr2)
which.min(summary(LM3)$bic)
which.min(summary(LM3)$cp)
coef(LM3, 31)
```

Optimal values for adjusted R squared and cp are obtained with 31 values while 27 variables are enough for bic. MAR and NUC are dropped. Rather than using proxies for test error, let us directly compute this error through validation set approach.

```{R}
train <- (D$time < 2015)
test <- (!train)
```

Simple linear regression on train now.

```{R}
tLM <- regsubsets(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + NUC + CPI + log(EMI) + MAR + geo, D, subset=train, nvmax = 36)
test.mat <- model.matrix(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + NUC + CPI + log(EMI) + MAR + geo, data=D[test,])
val.errors <- rep(NA, 36)
for (i in 1:36){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean((log(D$PRI[test]) - pred)^2)
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))
```

We include 34 variables, including all the non-geographical ones.

Now, time for ridge regression.

```{r}
library(glmnet)
x <- model.matrix(PRI ~ . - time, D)[,-1]
y <- D$PRI
set.seed(35)
ridge <- cv.glmnet(x[train,],y[train], alpha=0, standardize=T)
bestlam <- min(ridge$lambda)
```

What is the test MSE associated with this value of lambda?

```{r}
ridge.pred <- predict(ridge, s=bestlam, newx=x[test,])
mean((ridge.pred - y[test])^2)
```

First 25 best ridge model coefficients all refer to the geographical dummies.

```{r}
predict(ridge, type="coefficients", s=bestlam)[1:25,]
```

Time for lasso.

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha=1)
plot(lasso.mod)
```

Let us look for the best value of the parameter lambda (shrinkage)

```{r}
set.seed(35)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((pred - y[test])^2)
```

How many features is the model including?

```{r}
coef(cv.out, bestlam)
```

Few variables are excluded (including GDP and dependency (interestingly), and emissions).
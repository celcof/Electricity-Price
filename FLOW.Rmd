---
title: "STAT"
author: "Francesco Cabras"
date: "21 April 2020"
output:
  html_document:
    df_print: paged
---
```{r, echo=F}
library(ggpubr)
library(car)
library(eurostat)
library(HDCI)
library(caret)
```

Take data from library eurostat. Start from the dependent variable.

```{r}
X <- get_eurostat("ten00117", time_format = "num")
X <- X[grep("MSHH", X$indic_en), c(5:7)]
X <- X[!(X$geo %in% c("EU28", "NO", "TR", "EA", "EU27_2020", "AL", "BA", "GE", "IS", "LI", "MD", "ME", "MK", "RS", "UA", "XK")),]
names(X)[3] <- "PRI"
X$geo <- as.factor(as.character(X$geo))
```

Dependency from imports.

```{r}
I <- get_eurostat("nrg_ind_id", time_format = "num")
I <- I[grep("TOTAL", I$siec), ]
I <- I[,c(3:5)]
names(I)[3] <- "DEP"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Consumption of electricity.

```{r}
I <- get_eurostat("sdg_07_20", time_format = "num")
I <- I[, -1]
names(I)[3] <- "CON"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Real gdp per capita.

```{r}
I <- get_eurostat("sdg_08_10", time_format = "num")
I <- I[grep("CLV10_EUR_HAB", I$unit), ]
I <- I[, -c(1,2)]
names(I)[3] <- "GDP"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Oil price per barrel in dollars.

```{r}
time <- c(2008:2019)
OIL <- c(99.67,61.95,79.48,94.88,94.05,97.98,93.17,48.72,43.58,50.84,64.90,57.05)
I <- data.frame(time, OIL)
X <- merge(X, I, by="time", all.x=TRUE)
```

Share of renewable energy consumed.

```{r}
I <- get_eurostat("sdg_07_40", time_format = "num")
I <- I[I$nrg_bal == "REN", ]
I <- I[, -c(1,2)]
names(I)[3] <- "REN"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Nuclear energy production.

```{r}
search_eurostat("nuclear")$code
I <- get_eurostat("nrg_inf_nuc", time_format = "num")
I <- I[grep("PRD_NUCH", I$plant_tec), ]
I <- I[,c(3:5)]
names(I)[3] <- "NUC"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Price index

```{r}
I <- get_eurostat("prc_hicp_aind", time_format = "num")
I <- I[grep("CP00", I$coicop), ]
I <- I[grep("INX_A_AVG", I$unit), c(3:5)]
names(I)[3] <- "CPI"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

Emissions intensity.

```{r}
I <- get_eurostat("sdg_13_20", time_format = "num")
I <- I[,-1]
names(I)[3] <- "EMI"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
```

share of largest electricity generator in the market.

```{r}
I <- get_eurostat("ten00119", time_format = "num")
I <- I[,c(4:6)]
names(I)[3] <- "MAR"
X <- merge(X, I, by=c("time", "geo"), all.x=TRUE)
rm(I, OIL, time)
```

substitute NA with zero for nuclear capacity

```{r}
for (row in 1:length(X$NUC)){
  if(X$time[row] %in% c(2009:2018) & is.na(X$NUC[row])){
    X$NUC[row] <- 0
  }
}
for (country in levels(X$geo)){
  X[X$time == 2008 & X$geo == country, "NUC"] <- X[X$time == 2009 & X$geo == country, "NUC"]
}
rm(row)
```

substitute NA for Market Concentration

of Bulgaria. we miss values until 2012: we just substitute with first value available (2013)

```{r}
X[X$geo == "BG" & X$time %in% c(2008:2012), "MAR"] <- X[X$geo == "BG" & X$time == 2013, "MAR"]
```

Of Germany. goes from 28.4 in 2010 to 32.0 in 2013. we expect linear behaviour

```{r}
in2011 <- 28.4 + (32 - 28.4) / 3
in2012 <- in2011 + (32 - 28.4) / 3
X[X$geo == "DE" & X$time == 2011, "MAR"] <- in2011
X[X$geo == "DE" & X$time == 2012, "MAR"] <- in2012
rm(in2011, in2012)
```

Of Greece. in 2011, average between previous and subsequent years.

```{r}
X[X$geo == "EL" & X$time == 2011, "MAR"] <- mean(c(X[X$geo == "EL" & X$time == 2010, "MAR"], X[X$geo == "EL" & X$time == 2012, "MAR"]))
```

Of Luxembourg. We miss values until 2009: we just substitute with first value available (2010)

```{r}
X[X$geo == "LU" & X$time %in% c(2008:2009), "MAR"] <- X[X$geo == "LU" & X$time == 2010, "MAR"]
```

Of Austria. We miss values until 2010: we just substitute with first value available (2011). Also miss values from 2014 on. substitute with ones from 2013

```{r}
X[X$geo == "AT" & X$time %in% c(2008:2010), "MAR"] <- X[X$geo == "AT" & X$time == 2011, "MAR"]
X[X$geo == "AT" & X$time %in% c(2014:2018), "MAR"] <- X[X$geo == "AT" & X$time == 2013, "MAR"]
```

Of Netherlands: average year values from Belgium, Luxembourg and Germany. Also take a look at https://www.sciencedirect.com/science/article/pii/S0301421518308061

```{r}
for (i in 1:nrow(X)) {
  year <- X$time[i]
  if (X$geo[i] == "NL" && X$time[i] != 2019){
    DEvalue <- X[X$geo == "DE" & X$time == year, "MAR"]
    BEvalue <- X[X$geo == "BE" & X$time == year, "MAR"]
    LUvalue <- X[X$geo == "LU" & X$time == year, "MAR"]
    DKvalue <- X[X$geo == "DK" & X$time == year, "MAR"]
    NLvalue <- mean(c(DEvalue, BEvalue, LUvalue, DKvalue), na.rm=T)
    X[X$geo == "NL" & X$time == year, "MAR"] <- NLvalue
  }
  X$MAR <- round(X$MAR,1)
}
rm(DEvalue, BEvalue, LUvalue, DKvalue, NLvalue, i, year)
```

Of UK we miss values from 2014 on. substitute with ones from 2013.

```{r}
X[X$geo == "UK" & X$time %in% c(2014:2018), "MAR"] <- X[X$geo == "UK" & X$time == 2013, "MAR"]
```

Finally, sub na for EMI 2018 following the trend from 2016 and 2017 (value 2017 + (value 2017 - value 2016)).

```{r}
for (country in levels(X$geo)){
  X[X$time == 2018 & X$geo == country, "EMI"] <- X[X$time == 2017 & X$geo == country, "EMI"] + (X[X$time == 2017 & X$geo == country, "EMI"] - X[X$time == 2016 & X$geo == country, "EMI"])
}
rm(country)
```

Did not notice we also miss gdp poland for 2018. We assume that this value is following the trend of the last two years. value 2018 + diff(2017-2018)

```{r}
X[X$geo == "PL" & X$time == 2018, "GDP"] <- X[X$geo == "PL" & X$time == 2017, "GDP"] + (X[X$geo == "PL" & X$time == 2017, "GDP"] - X[X$geo == "PL" & X$time == 2016, "GDP"])
```

Missing too many values for 2019.

```{r}
D <- X[!X$time == 2019,]
rm(X)
```

Only country variability.

```{R}
COU <- lm(PRI ~ geo, D)
summary(COU)$r.squared
```

83.5% of the variability is due to differences between member states.
Let us also control for inflation, thinking it may suffice.

```{R}
COU <- lm(PRI ~ geo + CPI, D)
summary(COU)$r.squared
rm(COU)
```
87.5% of variability explained.

Some simple linear model.

```{R}
LM1 <- lm(PRI ~ DEP + CON + GDP + OIL + REN + NUC + CPI + EMI + MAR + geo, D)
summary(LM1)
```

Possible problems with assumptions, however. 

```{r}
plot(LM1)
```

For example, we expect nuclear production not to have a gaussian distribution, since most countries do not have nuclear reactors.

```{r}
ggdensity(D$NUC, main = "Density function NUC", xlab = "Nuclear Capacity")
```

Doesn't look gaussian at all. Test:

```{r}
shapiro.test(D$NUC)
```

Best thing could be to drop it, also because not that important.
Problem with some observations, specifically 116 and 144, which are records of Cyprus in 2012 and 2013.
In these two years the price of electricity skyrocketed for reasons that our model is not able to figure out (i.e., debt crisis).

```{r}
D[116,]
mean(D$PRI)
D[D$geo=="CY","PRI"]
D[c(116,144),]
```

To be checked:
- residuals sum to zero
- no autocorrelation for residuals
- no perfect multicollinearity

```{r}
mean(LM1$residuals)
lawstat::runs.test(LM1$residuals)
vif(LM1)
```
The mean of the residuals is ok, however we have some problems for autocorrelation of residuals and multicollinearity. The Variance Inflator Formula results in not promising values for GDP and NUC. We should not worry too much about this, however: if we remove the country-specific dummy variables, the vif values for the model turn out to be normal. The multicollinearity risk is also refuted when looking at the correlation matrix for the continuous variables: the NUC feature is not strongly correlated with any of the other variables.

```{r}
library(corrplot)
corrplot(cor(D[, 3:12]))
```

Correlation of GDP with Price and Quantity Consumed is understandable but why does NUC have such a high VIF value? Not clear.

Overall check.

```{r}
gvlma::gvlma(LM1)
```

Alternative. Take the log of PRI, CON, GDP, OIL and EMI and remove the two CY records.

```{r}
D <- D[!(D$time == 2012 & D$geo == "CY"),]
D <- D[!(D$time == 2013 & D$geo == "CY"),]
LM2 <- lm(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + CPI + log(EMI) + MAR + geo, D)
gvlma::gvlma(LM2)
```

Better behaviour. Normality of errors is not met, however.

```{r}
lawstat::runs.test(LM2$residuals)
```

Probably we should not be too worried, however. As taken from https://discovery.ucl.ac.uk/id/eprint/10070182/1/Schmidt_UCL_depos_JCE2018.pdf, " large sample size settings linear regression models are fairly robust to
violations of the normality assumption and hence arbitrary - bias inducing - outcome
transformations are usually unnecessary. "

This paper suggests that torturing the data for meeting the normality of error assumption usually makes things worse (when sample size is large enough, which means at least about 10 observations per parameter).

Subset selection. We want to maximize adjusted R squared and minimize cp and bic.

```{R}
library(leaps)
LM3 <- regsubsets(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + CPI + log(EMI) + MAR + geo, D, nvmax = 35)
which.max(summary(LM3)$adjr2)
which.min(summary(LM3)$bic)
which.min(summary(LM3)$cp)
coef(LM3, 31)
```

Optimal values for adjusted R squared and cp are obtained with 31 values while 27 variables are enough for bic. MAR and NUC are dropped. Rather than using proxies for test error, let us directly compute this error through validation set approach.

```{R}
train <- (D$time < 2015)
test <- (!train)
```

Simple linear regression on train now.

```{R}
tLM <- regsubsets(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + CPI + log(EMI) + MAR + geo, D, subset=train, nvmax = 35)
test.mat <- model.matrix(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + CPI + log(EMI) + MAR + geo, data=D[test,])
val.errors <- rep(NA, 35)
for (i in 1:35){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean((log(D$PRI[test]) - pred)^2)
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))
rm(tLM, val.errors, coefi, pred, test.mat)
```

We include 35 variables, including all the non-geographical ones.

Now, time for ridge regression.

```{r}
library(glmnet)
x <- model.matrix(log(PRI) ~ DEP + log(CON) + log(GDP) + log(OIL) + REN + CPI + log(EMI) + MAR + geo, D)[,-1]
y <- D$PRI
set.seed(35)
ridge <- cv.glmnet(x[train,],y[train], alpha=0, standardize=T)
bestlam <- min(ridge$lambda)
```

What is the test MSE associated with this value of lambda?

```{r}
ridge.pred <- predict(ridge, s=bestlam, newx=x[test,])
mean((ridge.pred - y[test])^2)
```

Coefficients.

```{r}
predict(ridge, type="coefficients", s=bestlam)[1:25,]
rm(ridge)
```

Time for lasso.

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha=1)
plot(lasso.mod)
```

Let us look for the best value of the parameter lambda (shrinkage).

```{r}
set.seed(35)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((pred - y[test])^2)
```

How many features is the model including?

```{r, warning=F}
C <- as.vector(coef(cv.out, bestlam))
length(C[C != 0])
rm(bestlam, pred, cv.out, ridge.pred, C)
```

All features are kept in the model (weird?).

```{r}
set.seed(35)
obj.escv <- escv.glmnet(x[train,], y[train])
obj <- Lasso(x[train,], y[train], lambda = obj.escv$lambda.cv)
length(obj$beta[obj$beta != 0])
pred <- mypredict(obj, newx = x[test,])
mean((pred - y[test])^2)
rm(obj.escv, obj, pred)
```

Similar result with different algorithm, so maybe not that weird. By the way, very good performance.
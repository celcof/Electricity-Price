---
title: "FLOW"
author: "Francesco Cabras"
date: "21 April 2020"
output:
  html_document:
    df_print: paged
---

Unbalanced data, not all countries and years present.

Novelty in the approach is the consideration of also countries external to EU.

```{r, echo=F}
library(ggpubr)
library(car)
library(eurostat)
library(HDCI)
library(caret)
library(scales)
library(glmnet)
library(lmtest)
```

Read csv.

```{r}
C <- read.csv("DAT.csv")
```

First thing: we notice that the dependent variable shows long tails, although this should not result as a big issue.

```{r}
ggdensity(C$PRI, main = "Density function PRI")
qqnorm(C$PRI)
qqline(C$PRI)
```
Experiments.

We build our model.

```{r}
train <- (C$time < 2015)
test <- (!train)
```

There are certain observations that are show quite a weird behaviour.
Let us go through them. 

```{r}
C[c(125,147),]
```

Observations 147 and 125, respectively the greatest and smallest, are likely to be mistaken, since they both come from Norway in 2003 and 2003.5: we simply decide to exclude them both.

```{r}
C <- C[-c(125,147),]

train <- (C$time < 2015)
test <- (!train)
```

Create dummy for geographical area. 

```{r}
for (row in 1:nrow(C)) {
  if (as.character(C[row, "geo"]) %in% c("BG", "RO", "CZ", "HU", "HR", "SE", "DK", "PL", "UK")) {
    C$EU[row] <- "EU"
}
  else if (as.character(C[row, "geo"]) %in% c("NO", "IS", "LI")) {
    C$EU[row] <- "EFTA"
  }
  else if (as.character(C[row, "geo"]) %in% c("BE", "EL", "LT", "PT", "ES", "LU", "FR", "SI", "MT", "SK", "DE", "IT", "NL", "FI", "EE", "CY", "AT", "IE", "LV")){
    C$EU[row] <- "EA"
    }
}
rm(row)

C$EU <- as.factor(C$EU)
```

Linear Model.

```{r}
LM <- lm(PRI ~ GAS + DEP + CON + EMI + MAR + REN + GDP + OIL + EU, C, subset=train)

summary(LM)
```

Situation does not look that bad, if we look at the plots for normality of errors and linearity assumption.

To be checked:
- no perfect multicollinearity

```{r}
library(corrplot)
corrplot(cor(C[, 3:11]))
```

Not all the features shall be significant: try to use only smaller subsets of the data.

```{R}
library(leaps)
nVar <- 7
LM2 <- regsubsets(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C, nvmax = nVar)
which.max(summary(LM2)$adjr2)
which.min(summary(LM2)$bic)
which.min(summary(LM2)$cp)
```

Which are the 2 most significant features?

```{r}
coef(LM2, 2)
```

To better compare accuracies of models we are going to use Mean Absolute Error (for interpretability reasons). Since our dependent variable is measured on a square root scale we have to have to square prediction vectors.

```{R}
tLM <- regsubsets(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C, subset=train, nvmax = nVar)
test.mat <- model.matrix(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, data=C[test,])
val.errors <- rep(NA, nVar)
for (i in 1:nVar){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean(abs(pred - C$PRI[test]))
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))
```

Now, time for ridge regression.

```{r}
x <- model.matrix(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C)[,-1]
y <- C$PRI
set.seed(35)
ridge <- cv.glmnet(x[train,], y[train], alpha=0, standardize=T)
bestlam <- min(ridge$lambda)
```

What is the test average difference between the prediction and the actual price of electricity?

```{r}
ridge.pred <- predict(ridge, s=bestlam, newx=x[test,])
mean(abs(ridge.pred - C$PRI[test]))
```


```{r}
predict(ridge, type="coefficients", s=bestlam)
```

Time for lasso.

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha=1)
plot(lasso.mod)
```

Let us look for the best value of the parameter lambda (shrinkage).

```{r}
set.seed(35)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean(abs(pred  - C$PRI[test]))
```

How many features is the model including?

```{r, warning=F}
V <- as.vector(coef(cv.out, bestlam))
length(V[V!= 0])
```

What we understood is that the error on the dependent variable is of about half a cent. This could seem as a good result but it also depends on what the mean change in electricity price is.

```{r}
mean(abs(C$PRI))
```

On average, the electricity price changes by half a cent as well so, to use one of our models or no model at all doesn't result in any difference in performance.
What if instead we wanted to guess, rather than the amount of change, at least the direction of the change.

```{r}
C$increase <- ifelse(C$PRI > 0, "Up", "Down")
C$increase <- as.factor(C$increase)

lR <- glm(increase ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C, family=binomial, subset=train)
pred <- predict(lR, C[test,], type="response")
glm.pred=rep("Down",434)
glm.pred[pred >.5]="Up"
table(prediction=glm.pred , groundtruth=C$increase[test])
mean(glm.pred == C$increase[test])
```

The algorithm is not really able to discriminate among observations with increasing and decreasing prices, since only 55% of the direction predictions are correct. In particular, the model overselects the upward increases and underselects the decreases in prices. This is a further indication that it is quite difficult to infer the trend in the price from the variables at hand.
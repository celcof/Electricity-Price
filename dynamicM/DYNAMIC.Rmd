---
title: "FLOW"
author: "Francesco Cabras"
date: "21 April 2020"
output:
  html_document:
    df_print: paged
---

Unbalanced data, not all countries and years present.

Novelty in the approach is in the consideration of also countries external to EU.

```{r, echo=F}
library(ggpubr)
library(car)
library(eurostat)
library(HDCI)
library(caret)
library(scales)
library(glmnet)
library(lmtest)
```

Read csv.

```{r}
C <- read.csv("DAT.csv")
```

First thing: we notice that the dependent variable is not really gaussian-distributed.

```{r}
ggdensity(C$PRI, main = "Density function PRI")
qqnorm(C$PRI)
qqline(C$PRI)
```
Experiments.

We build our model.

```{r}
train <- (C$time < 2015)
test <- (!train)
```

There are certain observations that are show quite a weird behaviour.
Let us go through them. 

```{r}
C[c(125,147),]
```

Observations 147 and 125, respectively the greatest and smallest, are likely to be mistaken, since they both come from Norway in 2003 and 2003.5: we simply decide to exclude them both.

```{r}
C <- C[-c(125,147),]

train <- (C$time < 2015)
test <- (!train)
```

```{r}
LM <- lm(PRI ~ GAS + DEP + CON + EMI + MAR + REN + GDP + OIL, C, subset=train)

summary(LM)
par(mfrow = c(2, 2))
plot(LM)
```

Situation does not look that bad, if we look at the plots for normality of errors and linearity assumption.


To be checked:
- no autocorrelation for residuals
- no perfect multicollinearity

```{r}
lawstat::runs.test(LM$residuals)
vif(LM)
```

The mean of the residuals is ok, however we have some problems for autocorrelation of residuals and multicollinearity. The Variance Inflator Formula results in not promising values for GDP and NUC. We should not worry too much about this, however: if we remove the country-specific dummy variables, the vif values for the model turn out to be normal. The multicollinearity risk is also refuted when looking at the correlation matrix for the continuous variables: the NUC feature is not strongly correlated with any of the other variables.

```{r}
library(corrplot)
corrplot(cor(C[, 3:11]))
```

We should clarify which variables are important and which instead are not.

```{r}

for (row in 1:nrow(C)) {
  if (as.character(C[row, "geo"]) %in% c("BG", "RO", "CZ", "HU", "HR", "SE", "DK", "PL", "UK")) {
    C$EU[row] <- "EU"
}
  else if (as.character(C[row, "geo"]) %in% c("NO", "IS", "LI")) {
    C$EU[row] <- "EFTA"
  }
  else if (as.character(C[row, "geo"]) %in% c("BE", "EL", "LT", "PT", "ES", "LU", "FR", "SI", "MT", "SK", "DE", "IT", "NL", "FI", "EE", "CY", "AT", "IE", "LV")){
    C$EU[row] <- "EA"
    }
}
rm(row)

C$EU <- as.factor(C$EU)
```

To be checked:
- residuals sum to zero
- no autocorrelation for residuals
- no perfect multicollinearity
- homoskedasticity

```{r}
LM <- lm(PRI ~ GAS + DEP + CON + EMI + MAR + REN + GDP + OIL + EU, C, subset=train)
par(mfrow = c(2, 2))
plot(LM)
```

Try to use different subsets of the data.

```{R}
library(leaps)
nVar <- 7
LM2 <- regsubsets(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C, nvmax = nVar)
which.max(summary(LM2)$adjr2)
which.min(summary(LM2)$bic)
which.min(summary(LM2)$cp)
```

Which are the 2 most significant features?

```{r}
coef(LM2, 2)
```

Simple linear regression on train now. To compare accuracies of models we are going to use Mean Absolute Error (for interpretability reasons). Since our dependent variable is measured on a square root scale we have to have to square prediction vectors.

```{R}
tLM <- regsubsets(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C, subset=train, nvmax = nVar)
test.mat <- model.matrix(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, data=C[test,])
val.errors <- rep(NA, nVar)
for (i in 1:nVar){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean(abs(pred - C$PRI[test]))
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))
```

Now, time for ridge regression.

```{r}
x <- model.matrix(PRI ~ DEP + CON + GDP + OIL + REN + EMI + MAR + EU, C)[,-1]
y <- C$PRI
set.seed(35)
ridge <- cv.glmnet(x[train,], y[train], alpha=0, standardize=T)
bestlam <- min(ridge$lambda)
```

What is the test average difference between the prediction and the actual price of electricity?

```{r}
ridge.pred <- predict(ridge, s=bestlam, newx=x[test,])
mean(abs(ridge.pred - C$PRI[test]))
```

Coefficients.

```{r}
predict(ridge, type="coefficients", s=bestlam)
```

Time for lasso.

```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha=1)
plot(lasso.mod)
```

Let us look for the best value of the parameter lambda (shrinkage).

```{r}
set.seed(35)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean(abs(pred  - C$PRI[test]))
```

How many features is the model including?

```{r, warning=F}
V <- as.vector(coef(cv.out, bestlam))
length(V[V!= 0])
```
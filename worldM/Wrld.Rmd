---
title: "Wrld"
author: "Francesco Cabras"
date: "10/21/2020"
output: html_document
---

```{r}
library(glmnet)
library(caret)
library(tidyverse)
library(leaps)

D <- read.csv("DAT.csv")
```

Simple linear model with country label.

```{r}
LM <- lm(PRI ~ EMI + DEP + CON + COA + REN + GDP + OILp + OILs, D)
plot(LM)
```

Points 714,716 and 718 correspond to records belonging to Solomon Islands. While in 2018 and 2019 the values are appropriate, the other years the recorded prices are way too high, almost 1 dollar per kWh. We decide to discard all Solomon Islands records.

```{r}
D <- D[-c(714,715,716,717,718),]
LM <- lm(PRI ~ EMI + DEP + CON + COA + REN + GDP + OILp + OILs, D)
```

Which are the most significant features inside the data set?

```{r}
summary(LM)
```

What if instead we also wanted to take into account the geographical position of the country itself: region is considered in the regression.

```{r}
LMc <- lm(PRI ~ EMI + DEP + CON + COA + REN + GDP + OILp + OILs + region, D)
summary(LMc)
```

Electricity is, on average, the most expensive in East Asia and Pacific and it is the cheapest in Europe and Central Asia.

Check the assumptions.

```{r}
par(mfrow = c(2, 2))
plot(LMc)
```

In the first plot on the top-left we see that the line is approximately horizontal so the linear relationship assumption holds.

Residuals are instead not as linear as we'd like. Another plot:

```{r}
library(olsrr)
ols_plot_resid_hist(LMc)
```

There is a sort of right-tail in our case. However, this behaviour was somehow predictable, since residuals are expected to be positive rather than negative (the average price is quite low, there are countries which spend a lot more than what they'd be supposed to but none that pay a lot less than they'd be supposed to).

```{r}
LMc <- lm(log(PRI) ~ EMI + DEP + log(CON) + COA + REN + log(GDP) + OILp + OILs + region, D)
ols_plot_resid_hist(LMc)
```

After using logarithmic transformation on the dependent and few independent variables situation improves.

```{r}
plot(LMc, 3)
```

Heteroskedasticity is not a problem.

```{r}
train <- (D$year < 2019)
test <- (!train)
```

Let us check performance on the validation set, which includes observations before 2015.

```{r}
D$region <- as.factor(D$region)
LMc <- lm(log(PRI) ~ EMI + DEP + log(CON) + COA + REN + log(GDP) + OILp + OILs + region, D, subset=train)
pred <- predict(LMc, D[test,])
mean(abs(D$PRI[test] - exp(pred)))
```

Our model, on average, gets it wrong for about 5 cents.
Ridge regression.

```{r}
x <- model.matrix(log(PRI) ~ EMI + DEP + log(CON) + COA + REN + log(GDP) + OILp + OILs + region, D)[,-1]
y <- D$PRI
set.seed(10)
ridge <- cv.glmnet(x[train,], log(y[train]), alpha=0, standardize=T)
bestlam <- min(ridge$lambda)
```

What is the test average difference between the prediction and the actual price of electricity?

```{r}
ridge.pred <- predict(ridge, s=bestlam, newx=x[test,])
mean(abs(exp(ridge.pred) - y[test]))
```

Performance does not improve using ridge regression.

Coefficients.

```{r}
predict(ridge, type="coefficients", s=bestlam)
```

Time for lasso.

```{r}
lasso.mod <- glmnet(x[train,], log(y[train]), alpha=1)
plot(lasso.mod)
```

Let us look for the best value of the parameter lambda (shrinkage).

```{r}
set.seed(10)
cv.out <- cv.glmnet(x[train,], sqrt(y[train]), alpha=1)
bestlam <- cv.out$lambda.min
pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean(abs(exp(pred) - y[test]))
```

About the same result as with standard linear model.

The elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods. The idea is to select the best alpha value through cross validation.

```{r}
set.seed(10)
Elastic <- train(
  log(PRI) ~ EMI + DEP + log(CON) + COA + REN + log(GDP) + OILp + OILs + region, data = D[train,], method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)

print(Elastic$bestTune)
pred <- predict(Elastic$finalModel, s=Elastic$bestTune$lambda, newx=x[test,])
mean(abs(exp(pred) - y[test]))
```

When tested on the validation set, the elastic net with alpha equal to 0.1 and a very small lambda value does not improve on the result of lasso.

Until now we have given for granted that the relationship between the independent and the dependent variables is linear. What happens if we included polynomials? Subset selection is used to analyze which are the most important features when also squared features are included.

```{r}

tLM <- regsubsets(log(PRI) ~ poly(EMI, 2) + poly(DEP,2) + poly(log(CON),2) + poly(COA,2) + poly(REN,2) + poly(log(GDP),2) + poly(OILp,2) + poly(OILs,2) + region, D[train,], nvmax = 15, really.big = T)

test.mat <- model.matrix(log(PRI) ~ poly(EMI, 2) + poly(DEP,2) + poly(log(CON),2) + poly(COA,2) + poly(REN,2) + poly(log(GDP),2) + poly(OILp,2) + poly(OILs,2) + region, data=D[test,])

val.errors <- rep(NA, 14)
for (i in 1:14){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean(abs(D$PRI[test] - exp(pred)))
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))

```

The best model including the polynomials contains 8 features with both the squared and the straight share of energy produced with coal and the share of energy produced with oil. However, accuracy on the validation set is inferior with respect to previous models.

```{r}

tLM <- regsubsets(log(PRI) ~ (EMI + DEP + log(CON) + COA + REN + log(GDP) + OILp + OILs)^2 + region, D[train,], nvmax = 30, really.big = T)

test.mat <- model.matrix(log(PRI) ~ (EMI + DEP + log(CON) + COA + REN + log(GDP) + OILp + OILs)^2 + region, data=D[test,])

val.errors <- rep(NA, 30)
for (i in 1:30){
  coefi <- coef(tLM, id=i)
  pred <- test.mat[,names(coefi)] %*% coefi
  val.errors[i] <- mean(abs(D$PRI[test] - exp(pred)))
}
which.min(val.errors)
val.errors[which.min(val.errors)]
coef(tLM, which.min(val.errors))

```

When we also include interactions we end up with a much complicated linear model which however performs very well.

